{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 align=\"center\">\n",
    "  Introduction to Neural Networks\n",
    "</h1>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mathematical Formalization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Purpose of training, validation, and test sets:\n",
    "   - Training: Used to train the model by updating weights.\n",
    "   - Validation: Used to tune hyperparameters and avoid overfitting.\n",
    "   - Test: Used to evaluate the final performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Influence of the number of examples (N):\n",
    "    - A larger N generally improves the generalization of the model by providing more data to learn patterns.\n",
    "    - Smaller N can lead to overfitting and poor generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Network Architecture (Forward Phase)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Importance of activation functions:\n",
    "   - They introduce non-linearity, enabling the network to learn complex patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Sizes (nx, nh, ny) in Figure 1:\n",
    "    - nx: Input size, nh: Hidden layer size, ny: Output size.\n",
    "    - These sizes depend on the data features and problem requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  ${y}$ vs. $\\hat{y}$:\n",
    "    - ${y}$: Ground truth label.\n",
    "    - $\\hat{y}$: Model prediction. Difference between them is captured by the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.  SoftMax usage:\n",
    "    - Converts raw outputs into probabilities summing to 1, useful for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.  Forward equations:\n",
    "    - $\\tilde{h}$ = ${W_h}$ $\\cdot$ ${x}$ + ${b_h}$\n",
    "    - ${h}$ = $\\tanh$($\\tilde{h}$)\n",
    "    - $\\tilde{y}$ = ${W_y}$ $\\cdot$ ${h}$ + ${b_y}$\n",
    "    - $\\hat{y}$ = SoftMax($\\tilde{y}$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cost Function<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Learning Method</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(nx, nh, ny):\n",
    "    params = {\n",
    "        \"Wh\": torch.randn(nh, nx) * 0.3,\n",
    "        \"bh\": torch.zeros(nh),\n",
    "        \"Wy\": torch.randn(ny, nh) * 0.3,\n",
    "        \"by\": torch.zeros(ny),\n",
    "    }\n",
    "    for key in params:\n",
    "        params[key].requires_grad = True\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, X):\n",
    "    h_pre = torch.mm(X, params[\"Wh\"].t()) + params[\"bh\"]\n",
    "    h = torch.tanh(h_pre)\n",
    "    y_pre = torch.mm(h, params[\"Wy\"].t()) + params[\"by\"]\n",
    "    y_exp = torch.exp(y_pre)\n",
    "    y_hat = y_exp / torch.sum(y_exp, dim=1, keepdim=True)\n",
    "    outputs = {\"h_pre\": h_pre, \"h\": h, \"y_pre\": y_pre}\n",
    "    return y_hat, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_accuracy(Yhat, Y):\n",
    "    _, Y_pred = torch.max(Yhat, 1)\n",
    "    _, Y_true = torch.max(Y, 1)\n",
    "    loss = -torch.mean(torch.sum(Y * torch.log(Yhat + 1e-10), dim=1))\n",
    "    accuracy = torch.mean((Y_pred == Y_true).float())\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, outputs, Y):\n",
    "    Yhat = torch.exp(outputs[\"y_pre\"])\n",
    "    Yhat /= torch.sum(Yhat, dim=1, keepdim=True)\n",
    "    grad_y_pre = Yhat - Y\n",
    "    grad_Wy = torch.mm(grad_y_pre.t(), outputs[\"h\"])\n",
    "    grad_by = torch.sum(grad_y_pre, dim=0)\n",
    "    grad_h = torch.mm(grad_y_pre, params[\"Wy\"])\n",
    "    grad_h_pre = grad_h * (1 - outputs[\"h\"] ** 2)\n",
    "    grad_Wh = torch.mm(grad_h_pre.t(), X)\n",
    "    grad_bh = torch.sum(grad_h_pre, dim=0)\n",
    "\n",
    "    return {\"Wy\": grad_Wy, \"by\": grad_by, \"Wh\": grad_Wh, \"bh\": grad_bh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, grads, eta):\n",
    "    for key in params:\n",
    "        params[key] -= eta * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(\n",
    "    X_train, Y_train, X_test, Y_test, nx, nh, ny, n_epochs=1000, batch_size=32, eta=0.01\n",
    "):\n",
    "    params = init_params(nx, nh, ny)\n",
    "    for epoch in range(n_epochs):\n",
    "        indices = torch.randperm(X_train.size(0))\n",
    "        X_train = X_train[indices]\n",
    "        Y_train = Y_train[indices]\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            X_batch = X_train[i : i + batch_size]\n",
    "            Y_batch = Y_train[i : i + batch_size]\n",
    "            Yhat, outputs = forward(params, X_batch)\n",
    "            loss, accuracy = loss_accuracy(Yhat, Y_batch)\n",
    "            grads = backward(params, outputs, Y_batch)\n",
    "            sgd(params, grads, eta)\n",
    "        if epoch % 100 == 0:\n",
    "            Yhat_test, _ = forward(params, X_test)\n",
    "            test_loss, test_accuracy = loss_accuracy(Yhat_test, Y_test)\n",
    "            print(\n",
    "                f\"Epoch {epoch}: Train Loss = {loss.item():.4f}, Test Accuracy = {test_accuracy.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Application to MNIST</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_on_circles():\n",
    "    np.random.seed(42)\n",
    "    def make_circles(n_samples=1000, noise=0.1, factor=0.5):\n",
    "        n = n_samples // 2\n",
    "        theta = np.linspace(0, 2*np.pi, n)\n",
    "        inner_x = factor * np.cos(theta)\n",
    "        inner_y = factor * np.sin(theta)\n",
    "        inner_labels = np.zeros(n)\n",
    "        outer_x = np.cos(theta)\n",
    "        outer_y = np.sin(theta)\n",
    "        outer_labels = np.ones(n)\n",
    "        X = np.vstack([\n",
    "            np.column_stack([inner_x, inner_y]),\n",
    "            np.column_stack([outer_x, outer_y])\n",
    "        ])\n",
    "        y = np.concatenate([inner_labels, outer_labels])\n",
    "        X += np.random.normal(0, noise, X.shape)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    X, y = make_circles(n_samples=1000, noise=0.1)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    svm = SVC(kernel='rbf')\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.RdYlBu)\n",
    "    plt.title('Training Data')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)\n",
    "    plt.title(f'SVM Decision Boundary (Accuracy: {accuracy:.2f})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"SVM Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return svm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Network on MNIST:\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)>\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error downloading train-images-idx3-ubyte.gz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     svm_model \u001b[38;5;241m=\u001b[39m train_svm_on_circles()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[92], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mMain function to run all implementations\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Neural Network on MNIST:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m mnist_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mnist_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining SVM on Circles Dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m train_svm_on_circles()\n",
      "Cell \u001b[0;32mIn[90], line 6\u001b[0m, in \u001b[0;36mtrain_mnist_neural_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mTrain neural network on MNIST dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load MNIST data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m mnist_data \u001b[38;5;241m=\u001b[39m \u001b[43mMNISTData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m X_train, Y_train, X_test, Y_test \u001b[38;5;241m=\u001b[39m mnist_data\u001b[38;5;241m.\u001b[39mget_data()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Neural network parameters for MNIST\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[89], line 19\u001b[0m, in \u001b[0;36mMNISTData.__init__\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     15\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1307\u001b[39m,), (\u001b[38;5;241m0.3081\u001b[39m,))\n\u001b[1;32m     16\u001b[0m ])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load MNIST datasets\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(\n\u001b[1;32m     26\u001b[0m     root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     27\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     28\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n",
      "File \u001b[0;32m/goinfre/obahi/deep_learning_tutorial/.venv/lib/python3.9/site-packages/torchvision/datasets/mnist.py:99\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/goinfre/obahi/deep_learning_tutorial/.venv/lib/python3.9/site-packages/torchvision/datasets/mnist.py:195\u001b[0m, in \u001b[0;36mMNIST.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error downloading train-images-idx3-ubyte.gz"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run all implementations\n",
    "    \"\"\"\n",
    "    print(\"Training Neural Network on MNIST:\")\n",
    "    mnist_model = train_mnist_neural_network()\n",
    "    \n",
    "    print(\"\\nTraining SVM on Circles Dataset:\")\n",
    "    svm_model = train_svm_on_circles()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
